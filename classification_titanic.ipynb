{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Examples - Titanic Dataset\n",
    "\n",
    "This lesson uses the [Titanic dataset](course_datasets.md#titanic).  It predicts Survival based on passenger class, sex, fare, embarkation, fare band, using logistic regression and decision tree classifiers.\n",
    "\n",
    "Steps\n",
    "* Load data into pandas\n",
    "* Clean data (select columns), remove any rows with missing values\n",
    "* Encode data (convert string columns into numbers, required by model). One-hot Ordinal (later) for passenger class\n",
    "* Encode label column (Died ->0, Survived ->1)\n",
    "* Split data into training and test sections\n",
    "* Build logistic regression model, fit on training data an predict on test data\n",
    "* Evaluate models with a confusion matrix\n",
    "* Build decision tree model, fit on training data and predict on test data. \n",
    "* Show decision tree model graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the titanic data from a CSV file on a public URL into a pandas DataFrame.  This version of the dataset has already been cleaned to some extent: it has columns `Adult Or Child`, and `Is Age Missing`, based on the Age column.  It also has a `FareBand` column based on the Fare column. The original Name column has been split into `Title`, `Surname` and `Other Names` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_url = 'https://raw.githubusercontent.com/MarkWilcock/CourseDatasets/main/Misc%20Datasets/Titanic%20Passenger.csv'\n",
    "df = pd.read_csv(titanic_url) # read the data\n",
    "df.head(5) # show the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Data\n",
    "\n",
    "Before modelling, it is good practice to explore the dataset. This helps us understand:\n",
    "- What columns are available and their data types\n",
    "- Whether there are missing values (non-null counts below the total row count)\n",
    "- The balance between classes (survived vs died) — an imbalanced dataset affects how we interpret accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show column names, data types, and count of non-null values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Values\n",
    "\n",
    "sklearn encoders cannot process `NaN` values. We drop any rows with missing values in our selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "print(f\"Rows remaining after dropping missing values: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance - are there roughly equal numbers of survivors and non-survivors?\n",
    "df['Survival'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the passenger class distribution\n",
    "df['Passenger Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the columns that are likely to be predictive of survival. We drop columns like `Name` and `Ticket Number` that are unique to individual passengers and would not generalise to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slim = df[['Survival', 'Title', 'Passenger Class', 'Gender', 'Embarked', 'FareBand', 'Adult Or Child']]\n",
    "df_slim.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename columns to use a consistent `snake_case` style, which is the Python convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slim.columns = ['survival', 'title', 'pass_class', 'gender', 'embarked', 'fareband', 'adult_or_child']\n",
    "df_slim.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Features\n",
    "\n",
    "Machine learning models require numeric inputs. We need to convert our text columns into numbers.\n",
    "\n",
    "**One-hot encoding** converts a categorical column into several binary (0/1) columns — one per category. For example, the `gender` column becomes two new columns:\n",
    "\n",
    "| gender | → | gender_male | gender_female |\n",
    "|--------|---|:-----------:|:-------------:|\n",
    "| male   |   | 1           | 0             |\n",
    "| female |   | 0           | 1             |\n",
    "\n",
    "This avoids implying any numeric ordering between categories (which simple integer encoding like `male=0, female=1` would incorrectly suggest).\n",
    "\n",
    "See [this explainer article](https://www.geeksforgeeks.org/ml-one-hot-encoding/) and the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns = ['title', 'gender', 'embarked', 'fareband', 'adult_or_child']\n",
    "categorical_encoders = OneHotEncoder(sparse_output=False)\n",
    "categorical_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ordinal encoding** is used when a categorical column has a meaningful rank order. Passenger class is ordinal — 1st is \"better\" than 2nd, which is \"better\" than 3rd — so we encode it as numbers that preserve this order:\n",
    "\n",
    "| pass_class | Encoded value |\n",
    "|:----------:|:-------------:|\n",
    "| 1st        | 0             |\n",
    "| 2nd        | 1             |\n",
    "| 3rd        | 2             |\n",
    "\n",
    "We specify the order explicitly with `categories=[pass_class_values]` so the encoder knows the intended ranking rather than guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_columns =  ['pass_class']\n",
    "pass_class_values = ['1st', '2nd', '3rd']\n",
    "ordinal_encoders = OrdinalEncoder(categories=[pass_class_values]) \n",
    "ordinal_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ColumnTransformer lets us assemble the transforms on all the dataset columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(transformers = [\n",
    "        ('cat', categorical_encoders, category_columns),\n",
    "        ('ord', ordinal_encoders, ordinal_columns)\n",
    "        ], \n",
    "        remainder = 'drop')\n",
    "ct.set_output(transform='pandas')\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is the standard name for the transformed data of features (independent variables).  Notice that `X` has more columns than our original features. One-hot encoding expands each categorical column into multiple binary columns — one per unique category value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ct.fit_transform(df_slim)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original feature count: {len(df_slim.columns) - 1} columns\")\n",
    "print(f\"Transformed feature count: {X.shape[1]} columns\")\n",
    "print(\"The increase is due to one-hot encoding creating one binary column per category value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an array of labels from the survival column.  survival is a text column (with values Died and Survived), and is transformed to an array of numbers either 0 (Died) and 1 (Survived)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df_slim['survival'])\n",
    "y[:5] # show the first 5 elements of y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split\n",
    "\n",
    "We split the data into two sets:\n",
    "\n",
    "- **Training set (80%)** — the model learns patterns from this data\n",
    "- **Test set (20%)** — used *after* training to measure how well the model generalises to data it has never seen\n",
    "\n",
    "This prevents us from reporting inflated accuracy from a model that has simply memorised the training data (known as **overfitting**).\n",
    "\n",
    "`random_state=42` fixes the random seed so the split is reproducible — everyone running this notebook gets the same split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the transformed feature columns. Notice that the column count is larger than our original 6 features — this is because one-hot encoding replaces each categorical column with multiple binary columns (one per category value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformed feature columns:\")\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression model\n",
    "Build and fit the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR = LogisticRegression()\n",
    "model_LR.fit(X_train, y_train)\n",
    "predictions_LR = model_LR.predict(X_test)\n",
    "predictions_LR[:5] # show the first 5 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Logistic Regression Model\n",
    "\n",
    "We use several metrics to understand model performance:\n",
    "\n",
    "- **Accuracy** — proportion of all predictions that were correct\n",
    "- **Precision** — of all passengers predicted to survive, what fraction actually did?\n",
    "- **Recall** — of all passengers who actually survived, what fraction did we correctly predict?\n",
    "- **F1 score** — the harmonic mean of precision and recall; useful when classes are imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification Report\\n',  classification_report(y_test, predictions_LR))\n",
    "print(f'f1 score\\n {f1_score(y_test, predictions_LR):3.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "The confusion matrix breaks down predictions into four categories:\n",
    "\n",
    "|                      | Predicted: Died | Predicted: Survived |\n",
    "|----------------------|:---------------:|:-------------------:|\n",
    "| **Actual: Died**     | True Negative   | False Positive      |\n",
    "| **Actual: Survived** | False Negative  | True Positive       |\n",
    "\n",
    "- **False Positives** — predicted survived but actually died (Type I error)\n",
    "- **False Negatives** — predicted died but actually survived (Type II error)\n",
    "\n",
    "The diagonal (top-left, bottom-right) shows correct predictions. Off-diagonal cells are errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, predictions_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model\n",
    "\n",
    "A **decision tree** classifies passengers by learning a series of yes/no rules from the training data (e.g. *\"Was the passenger female? If yes, did they travel in 1st or 2nd class?\"*). The result is a tree structure that is intuitive and easy to visualise.\n",
    "\n",
    "We limit the tree depth with `max_depth=4` to prevent **overfitting** — without a limit, the tree would memorise the training data by creating very specific rules that don't generalise to new passengers.\n",
    "\n",
    "> **Try it:** Remove `max_depth=4` and re-run. How does the accuracy on the test set change? What does the tree look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DT = DecisionTreeClassifier(max_depth=4)\n",
    "model_DT.fit(X_train, y_train)\n",
    "model_DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_DT = model_DT.predict(X_test)\n",
    "predictions_DT[:5] # show the first 5 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision Tree — Classification Report\\n', classification_report(y_test, predictions_DT))\n",
    "print('Decision Tree — Confusion Matrix')\n",
    "print(confusion_matrix(y_test, predictions_DT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the decision tree model using scikit-learn's built-in `plot_tree` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "plot_tree(model_DT,\n",
    "          feature_names=X.columns,\n",
    "          class_names=['Died', 'Survived'],\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persist the model in case we want to rerun without retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(model_DT, 'outputs/titanic_model.pkl')\n",
    "print(\"Model saved to outputs/titanic_model.pkl\")\n",
    "print(\"Use joblib.load('outputs/titanic_model.pkl') to reload it later without retraining.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison\n",
    "\n",
    "Which model performed better? Consider:\n",
    "- **Accuracy** tells you overall correctness, but can be misleading with imbalanced classes\n",
    "- **F1 score** balances precision and recall — more informative here since more passengers died than survived\n",
    "- **Interpretability** — decision trees are easy to explain to non-technical stakeholders; logistic regression coefficients are harder to visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Model Comparison ===\\n\")\n",
    "print(f\"Logistic Regression — Accuracy: {accuracy_score(y_test, predictions_LR):.3f}   F1: {f1_score(y_test, predictions_LR):.3f}\")\n",
    "print(f\"Decision Tree       — Accuracy: {accuracy_score(y_test, predictions_DT):.3f}   F1: {f1_score(y_test, predictions_DT):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "1. **Loaded and explored** the Titanic passenger dataset\n",
    "2. **Selected features** likely to be predictive of survival and dropped rows with missing values\n",
    "3. **Encoded** categorical features (one-hot encoding) and ordinal features (ordinal encoding), and encoded the target label\n",
    "4. **Split** the data into training (80%) and test (20%) sets\n",
    "5. **Trained** two classifiers: Logistic Regression and Decision Tree\n",
    "6. **Evaluated** both models using accuracy, F1 score, classification report, and confusion matrix\n",
    "\n",
    "### What to Try Next\n",
    "\n",
    "- Add or remove features (columns) and see how model performance changes\n",
    "- Try `RandomForestClassifier` — an ensemble of many decision trees that often outperforms a single tree\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
