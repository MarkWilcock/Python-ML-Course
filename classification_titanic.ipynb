{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Examples - Titanic Dataset\n",
    "\n",
    "This lesson uses the [Titanic dataset](course_datasets.md#titanic).  It predicts Survival based on passenger class, sex, fare, embarkation, fare band, using logistic regression and decision tree classifiers.\n",
    "\n",
    "Steps\n",
    "* Load data into pandas\n",
    "* Clean data (select columns), remove any rows with missing values\n",
    "* Encode data (convert string columns into numbers, required by model). One-hot Ordinal (later) for passenger class\n",
    "* Encode label column (Died ->0, Survived ->1)\n",
    "* Split data into training and test sections\n",
    "* Build logistic regression model, fit on training data an predict on test data\n",
    "* Evaluate models with a confusion matrix\n",
    "* Build decision tree model, fit on training data and predict on test data. \n",
    "* Show decision tree model graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the titanic data from a CSV file on a public URL into a pandas  DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_url = 'https://raw.githubusercontent.com/MarkWilcock/CourseDatasets/main/Misc%20Datasets/Titanic%20Passenger.csv'\n",
    "df = pd.read_csv(titanic_url) # read the data\n",
    "df.head(5) # show the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the columns of interest, and rename these in a consistent snake_case (Pythonic) style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slim = df.loc[:, ['Survival', 'Title','Passenger Class','Gender', 'Embarked', 'FareBand', 'Adult Or Child']]\n",
    "df_slim.columns = ['survival', 'title','pass_class','gender', 'embarked', 'fareband', 'adult_or_child']\n",
    "df_slim.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the categorical columns with a one hot encoder. See [this explainer article](https://www.geeksforgeeks.org/ml-one-hot-encoding/) and the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns = ['title', 'gender', 'embarked', 'fareband', 'adult_or_child']\n",
    "categorical_encoders = OneHotEncoder(sparse_output=False)\n",
    "categorical_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinal data are categorical data with a natural rank order but the distances between the categories are uneven or unknown. e.g. cool, warm, hot.  In this dataset, the pass_class (passenger class) and arguably fareband columns contain ordinal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_columns =  ['pass_class']\n",
    "pass_class_values = ['1st', '2nd', '3rd']\n",
    "#fareband_values = ['0 - 10', '10 - 20', '20 - 30', '30 - above']\n",
    "ordinal_encoders = OrdinalEncoder(categories=[pass_class_values]) \n",
    "ordinal_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ColumnTransformer lets us assemble the transforms on all the dataset columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(transformers = [\n",
    "        ('cat', categorical_encoders, category_columns),\n",
    "        ('ord', ordinal_encoders, ordinal_columns)\n",
    "        ], \n",
    "        remainder = 'drop')\n",
    "ct.set_output(transform='pandas')\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is the standard name for the transformed data of features (independent variables)\n",
    "X = ct.fit_transform(df_slim)\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an array of labels from the survival column.  survival is a text column (with values Died and Survived), and is transformed to an array of numbers either 0 (Died) and 1 (Survived)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df_slim.loc[:, 'survival'])\n",
    "y[:5] # show the first 5 elements of y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression model\n",
    "Build and fit the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR = LogisticRegression()\n",
    "model_LR.fit(X_train, y_train)\n",
    "predictions_LR = model_LR.predict(X_test)\n",
    "predictions_LR[:5] # show the first 5 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate using standard metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification Report\\n',  classification_report(y_test, predictions_LR))\n",
    "print(f'f1 score\\n {f1_score(y_test, predictions_LR):3.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand how well the model is performing with a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, predictions_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model\n",
    "Build and fit the decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DT = DecisionTreeClassifier(max_depth=4)\n",
    "model_DT.fit(X_train, y_train)\n",
    "model_DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_DT = model_DT.predict(X_test)\n",
    "predictions_DT[:5] # show the first 5 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predictions_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install an extension such as GraphViz Interactive Preview, to view the decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.export_graphviz(model_DT, \n",
    "                      out_file='outputs/titanic_tree.dot', \n",
    "                      feature_names=X.columns, \n",
    "                      class_names=['Died', 'Survived'],\n",
    "                      label='all',\n",
    "                      rounded=True,\n",
    "                      filled=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persist the model in case we want to rerun without retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model_DT, 'outputs/titanic_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old code - ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_slim_no_label = df_slim.drop('survival', axis=1)\n",
    "# df_slim_no_label.head(5) # show the first 5 rows\n",
    "# y_DT = df_slim['survival'].apply(lambda x: 1 if x == 'Survived' else 0)\n",
    "# y_DT.head()\n",
    "# X_DT = pd.get_dummies(df_slim_no_label) \n",
    "# X_DT.head(5)\n",
    "# X_train_DT, X_test_DT, y_train_DT, y_test_DT = train_test_split(X_DT, y_DT, test_size=0.2)\n",
    "# (X_train_DT.shape, y_train_DT.shape), (X_test_DT.shape, y_test_DT.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
