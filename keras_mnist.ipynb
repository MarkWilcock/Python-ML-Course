{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras example - MNIST dataset\n",
    "\n",
    "This lesson uses the [MNIST digits dataset](course_datasets.md#mnist-digits).  The keras documentation has a similar tutorial [here](https://keras.io/examples/vision/mnist_convnet/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Required installations\n",
    "# !pip install tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport keras\nfrom keras import layers\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is an internal dataset to the keras package.  The labels (y) are values between 0 and 9. Load the data into training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "print(f'X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape}, X_test.shape: {X_test.shape}, y_test.shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the first (of 60,000) training images.  It is a 28 x 28 array of values between 0 (black) and 255 (white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the image using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some more of these images. The next code cell finds the first image of each class (0 through 9) in the training data and plots it with the label above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, num_classes, figsize=(20,20))  \n",
    "\n",
    "# Loop through 10 elements from train dataset \n",
    "for i in range(num_classes): # 0 to 9\n",
    "  sample = X_train[y_train == i][0] # Get first image from each class\n",
    "  ax[i].imshow(sample, cmap=\"gray\") # Show sample image\n",
    "  ax[i].set_title(f\"Label:{i}\") # Set title as class label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to one-hot encode the y values so that each value (digit) becomes a vector of 10 values (9 values of zero and a single 1). For example 2 becomes [0,0,1,0,0,0,0,0,0,0].\n",
    "\n",
    "One-hot encoding is a technique used in machine learning and data preprocessing to represent categorical data as binary vectors. It is particularly useful when dealing with categorical features or labels in a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    print(f\"Before : {y_train[i]}\")\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "for i in range(7):\n",
    "    print(f\"After : {y_train[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalise the data to ensure values are between 0 and 1.  This ensure that with several variable with different ranges, one variable does not dominate the ML calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "X_train[0] # now has values between 0 and 1 rather than 0 and 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train =np.expand_dims(X_train, -1)\n",
    "X_test =np.expand_dims(X_test, -1)\n",
    "print(f'X_train.shape:\\n{X_train.shape}\\nX_test.shape:\\n{X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model using a Convolutional Neural Network (CNN).\n",
    "\n",
    "**How it works:**\n",
    "1. **Conv2D layers** scan the image with sliding filters to detect patterns (like curves, edges, shapes)\n",
    "2. **MaxPooling2D** shrinks the image to keep only the most important information\n",
    "3. **Flatten** converts the processed image into a single list of numbers\n",
    "4. **Dropout** randomly turns off some neurons to prevent the model from memorizing the training data\n",
    "5. **Dense (output)** makes the final prediction: which digit (0-9) is in the image\n",
    "\n",
    "**In simple terms:** The model learns to recognize handwritten digits by extracting features from the image and passing them through layers that gradually learn what each digit looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model against the training date.  This sets the weights between nodes at the bias for each node to mimise also function between the values and the training y values.  \n",
    "\n",
    "BATCH_SIZE and epochs control how long the proicess takes - these value are fairly arbitary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "epochs = 11\n",
    "model.fit(x=X_train, y=y_train, batch_size = BATCH_SIZE, epochs = epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)\n",
    "print(f'predicted.shape: {predicted.shape}\\n first value:\\n{predicted[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of how argmax works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prediction = predicted[0]\n",
    "print(f'first_prediction: {first_prediction}')\n",
    "first_actual = np.argmax(first_prediction)\n",
    "print(f'first_actual: {first_actual}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = np.argmax(y_test, axis=1)\n",
    "y_pred_classes = np.argmax(predicted, axis=1)\n",
    "print(f'y_actual shape: {y_actual.shape}, y_pred_classes shape: {y_pred_classes.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the image, and label with the actual and predicted values of the ith test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(X_test[i].reshape(28, 28), cmap=\"gray\")\n",
    "plt.title(f\"Actual: {y_actual[i]}, Predicted: {y_pred_classes[i]}\")\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how well the model does against the training and more importantly the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF TUTORIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows what the softmax algorithm does - chooses the biggest of the 10 probability values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1) # pick the highest probability with \"np.argmax()\", and turn it into an index uing \"axis=1\"\n",
    "\n",
    "# print vector of probabilities\n",
    "print(f\"What Softmax predicted: {y_pred}\")\n",
    "\n",
    "# print predicted number\n",
    "print(f\"What Softmax actually means: {y_pred_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_num = np.random.choice(len(X_test))\n",
    "X_sample = X_test[random_num]\n",
    "\n",
    "# save true label of this sample in a variable\n",
    "y_actual = np.argmax(y_test, axis=1)\n",
    "y_sample_actual = y_actual[random_num]\n",
    "\n",
    "# save a predicted label of this sample in a variable\n",
    "y_sample_pred_class = y_pred_classes[random_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(f\"random_num {random_num}  Predicted: {y_sample_pred_class}, True:{y_sample_actual}\")\n",
    "plt.imshow(X_sample.reshape(28, 28), cmap=\"gray\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}